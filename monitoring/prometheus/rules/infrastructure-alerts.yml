# ═══════════════════════════════════════════════════════════════
# BaseBook DEX - Infrastructure Alert Rules
# ═══════════════════════════════════════════════════════════════

groups:
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # ═════════════════════════════════════════════════════════
      # SERVICE AVAILABILITY
      # ═════════════════════════════════════════════════════════
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          page: "true"
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.instance }} has been down for more than 1 minute"
          impact: "Service unavailable to users"
          action: "Check pod status, logs, and recent deployments"

      - alert: ServiceFlapping
        expr: |
          changes(up[15m]) > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Service {{ $labels.job }} is flapping"
          description: "Service has restarted {{ $value }} times in 15 minutes"
          impact: "Unstable service, intermittent availability"
          action: "Check for OOMKills, crashes, or health check issues"

      # ═════════════════════════════════════════════════════════
      # POD HEALTH
      # ═════════════════════════════════════════════════════════
      - alert: PodRestartingFrequently
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) * 60 > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} is restarting frequently"
          description: "Pod has restarted {{ $value }} times per minute"
          impact: "Service instability"
          action: "Check pod logs for crash reasons"

      - alert: PodCrashLooping
        expr: |
          rate(kube_pod_container_status_restarts_total[5m]) > 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Pod {{ $labels.pod }} is crash looping"
          description: "Pod is continuously restarting"
          impact: "Service degradation or unavailability"
          action: "Check logs, events, and resource limits"

      - alert: PodNotReady
        expr: |
          kube_pod_status_phase{phase!~"Running|Succeeded"} > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} not ready"
          description: "Pod is in {{ $labels.phase }} phase"
          impact: "Reduced capacity or pending deployment"
          action: "Check pod events and describe pod"

      # ═════════════════════════════════════════════════════════
      # CONTAINER RESOURCES
      # ═════════════════════════════════════════════════════════
      - alert: ContainerHighCPU
        expr: |
          (
            sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod, container)
            /
            sum(container_spec_cpu_quota{container!=""} / container_spec_cpu_period{container!=""}) by (pod, container)
          ) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.container }} in {{ $labels.pod }} has high CPU"
          description: "CPU usage is {{ $value }}% of limit"
          impact: "Performance degradation, risk of throttling"
          action: "Consider increasing CPU limits or scaling"

      - alert: ContainerHighMemory
        expr: |
          (
            sum(container_memory_working_set_bytes{container!=""}) by (pod, container)
            /
            sum(container_spec_memory_limit_bytes{container!=""} > 0) by (pod, container)
          ) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.container }} in {{ $labels.pod }} has high memory"
          description: "Memory usage is {{ $value }}% of limit"
          impact: "Risk of OOMKill"
          action: "Check for memory leaks or increase limits"

      - alert: ContainerOOMKilled
        expr: |
          increase(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[5m]) > 0
        labels:
          severity: critical
        annotations:
          summary: "Container {{ $labels.container }} was OOMKilled"
          description: "Container ran out of memory and was killed"
          impact: "Service disruption"
          action: "Increase memory limits or fix memory leak"

      # ═════════════════════════════════════════════════════════
      # DATABASE
      # ═════════════════════════════════════════════════════════
      - alert: DatabaseDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          page: "true"
        annotations:
          summary: "PostgreSQL database is down"
          description: "Database connection failed"
          impact: "Service completely unavailable"
          action: "Check database pod/service status immediately"

      - alert: DatabaseHighConnections
        expr: |
          sum(pg_stat_database_numbackends) / pg_settings_max_connections * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "{{ $value }}% of max connections used"
          impact: "New connections may be rejected"
          action: "Check for connection leaks or increase max_connections"

      - alert: DatabaseSlowQueries
        expr: |
          rate(pg_stat_statements_mean_exec_time_seconds[5m]) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Database queries are slow"
          description: "Average query time is {{ $value | humanizeDuration }}"
          impact: "API latency increased"
          action: "Check slow query log and optimize"

      - alert: DatabaseReplicationLag
        expr: |
          pg_replication_lag_seconds > 60
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database replication lag detected"
          description: "Replication lag is {{ $value | humanizeDuration }}"
          impact: "Read replicas serving stale data"
          action: "Check replication health"

      # ═════════════════════════════════════════════════════════
      # REDIS
      # ═════════════════════════════════════════════════════════
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          page: "true"
        annotations:
          summary: "Redis is down"
          description: "Redis connection failed"
          impact: "Cache unavailable, performance degraded"
          action: "Check Redis pod/service status"

      - alert: RedisHighMemoryUsage
        expr: |
          redis_memory_used_bytes / redis_memory_max_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage is high"
          description: "{{ $value }}% of max memory used"
          impact: "Risk of evictions or OOM"
          action: "Review cache TTLs or increase memory"

      - alert: RedisHighEvictionRate
        expr: |
          rate(redis_evicted_keys_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Redis key eviction rate"
          description: "{{ $value }} keys evicted per second"
          impact: "Reduced cache effectiveness"
          action: "Increase Redis memory or review cache strategy"

      - alert: RedisSlowCommands
        expr: |
          redis_slowlog_length > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis slow commands detected"
          description: "{{ $value }} commands in slow log"
          impact: "Cache performance degraded"
          action: "Review slow log and optimize commands"

      # ═════════════════════════════════════════════════════════
      # NETWORK
      # ═════════════════════════════════════════════════════════
      - alert: HighNetworkErrors
        expr: |
          rate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High network error rate on {{ $labels.device }}"
          description: "{{ $value }} errors per second"
          impact: "Potential packet loss"
          action: "Check network interface health"

      - alert: HighNetworkBandwidthUsage
        expr: |
          (
            rate(node_network_receive_bytes_total[5m]) + rate(node_network_transmit_bytes_total[5m])
          ) / 1024 / 1024 > 800
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High network bandwidth usage"
          description: "{{ $value }} MB/s throughput"
          impact: "Possible network saturation"
          action: "Check for unusual traffic patterns"

      # ═════════════════════════════════════════════════════════
      # DISK
      # ═════════════════════════════════════════════════════════
      - alert: DiskSpaceLow
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            /
            node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Disk space low on {{ $labels.instance }}"
          description: "Only {{ $value }}% available"
          impact: "Risk of disk full errors"
          action: "Clean up logs or increase disk size"

      - alert: DiskSpaceCritical
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            /
            node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 10
        for: 2m
        labels:
          severity: critical
          page: "true"
        annotations:
          summary: "CRITICAL: Disk space critically low"
          description: "Only {{ $value }}% available"
          impact: "Imminent service failure"
          action: "Immediate cleanup or disk expansion required"
